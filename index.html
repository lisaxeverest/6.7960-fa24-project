<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>
	
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


	  <title>Prescriptive neural networks for liver trauma patients</title>
      <meta property="og:title" content="Prescriptive neural networks for liver trauma patients" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Prescriptive neural networks for liver trauma patients</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://lisaxeverest.github.io">Lisa Everest</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Carol (Xiaomiao) Gao</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction and Motivation</a><br><br>
              <a href="#related_work">Related work</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#discussion">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/liver-laceration-grades.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
					CT scans of the 5 grades of liver trauma <a href="https://www.researchgate.net/publication/361478611/figure/fig1/AS:1169964820119558@1655952758942/Different-types-of-liver-trauma-a1cm-depth-laceration-or-10-surface-area-haematoma.jpg">(credit)</a>
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction and Motivation</h1>
            Motivate your project. What question are you asking. Why is it unanswered so far? What gap in the literature or practice are you filling?
            Why is it important?
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<div class="content-margin-container" id="related_work">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Related work</h1>
				The most traditional method to personalized treatment is Regress and Compare in which a regression is trained on each set of training data by their treatment. 
				A given observation is then evaluated on models trained from all treatment options and the treatment that optimizes its outcome is selected. 
				<br><br> 
				
				Recent literature in personalized decision-making has primarily leveraged tree-based machine learning methods and causal methods to improve on regress and compare. 
				<a href="#ref_1">[1]</a> extends from Regress and Compare and proposes a single-task method that recursively partitions the input space with the smallest sum of impurities.
				<a href="#ref_2">[2]</a> extends the Optimal Classification Trees in <a href="#ref_3">[3]</a> - an interpreatble tree-based method - to prescription. 
				They introduce Optimal Prescriptive Trees which estimates the counterfactual outcome of each observation under each treatment during training 
				to minimize both the prediction and the prescription error. 
				Relately, <a href="#ref_4">[4]</a>

				<br><br> 
				More recently, there has been emerging literature on combining prescription and neural networks. <a href="#ref_5">[5]</a> proposes a prescriptive neural network model 
				using ReLU as the activation function. 
				In particular, they partition the sample space into disjoint polyhedra and assign a treament to all data points that belong in each partition. 
				<a href="#ref_6">[6]</a> formulates and solves an optimization problem using predictions obtained through a neural network in the objective function. 
				They also restrict to ReLU as the activation function to allow modelling as a network flow problem. 
				<br><br> 
				We contribute to this strand of literature by relaxing the restriction to ReLU and allowing any type of activation functions in the neural network. 
				Moreover, we solve the prescription problem that optimizes the objective simultaneously as training the neural network.


		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
		</div><div class="content-margin-container" id="Methodology">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
				<h2>Methodology</h2>
				We divide the training process into four main steps: embedding extraction, counterfactual estimation, and 
				prescription policy learning (neural network training). <br><br>
				<h3>Embedding extraction</h3>
				The first step in the model pipeline is to extract embeddings from the structured and unstructured data. <br>
				<h4>Structured data</h4> 
				We extract embeddings from structured feature data through traditional preprocessing techniques as 
				described below, where the technique depends on whether the feature is numerical, categorical, or ordinal.
				<ol>
					<li>
						<b>Numerical features.</b> We normalize numerical features to the interval [0,1] for the 
						counterfactual estimation and model training steps. This increases stability and helps to weight features equally!
					</li>
					<li>
						<b>Categorical features.</b> For categorical features, we use one-hot encodings to convert them to 
						binary features, such that each category becomes a new indicator feature.
					</li>
					<li><b>Ordinal features</b> Ordinal features are categorical features whose values carry numerical information. 
						Since these categories have a natural order to them, we can assign each category a number such as 1 to 5, 
						where relative magnitude holds information. For example, 
						a feature, "Age Groups", may take on the following values: "0-18", "19-24", "25-49", "50-74", "75-100"; we 
						can assign these to numbers 1 - 5, such that "0-18" is assigned 1, and so on. That way, an individual with an age of 
						"1" in the preprocessed data indicates that they are younger relative to an individual with an age of "3". 
						We then treat these ordinal features 
						as numerical features in our experiments.
					</li>
				</ol>
				<h4>Unstructured data</h4>
				We extract embeddings from unstructured data using pretrained, deep learning models. By passing each observation's 
				unstructured data through these pretrained models, we can obtain a vector representation of the unstructured datapoint. 
				In particular, for our particular use case in liver trauma, we use Clinical Longformer, 
				a long sequence transformer model trained via a sparse attention mechanism on domain-specific, large-scale clinical corpora 
				<a href="#ref_7">[7]</a>; from this model, we obtain a 768-dimensional embedding vector for each observation's clinical note data. <br><br>

				A 768-dimensional embedding vector, however, is very large for training a neural network with only FIXME datapoints. In order to 
				improve stability, tractability, and performance, we therefore 
				apply Principle Component Analysis (PCA) to reduce the dimensions of the clinical note embeddings. 
				We tried reducing to a 32-dimensional representation, and it works well for the liver trauma dataset. <br><br>

				While we specifically use ClinicalLongformer, we observe that any pretrained large language model (LLM) may be used to process 
				unstructured text data. This makes it easy to update the model with improved, future LLMs, as well as flexible in application to 
				other datasets and use cases. Additionally, we want to point out that this methodology can easily be applied to image data, by 
				utilizing any pretrained computer vision (CV) model to process unstructured image data and obtain image embeddings. This results in 
				an embedding extraction step for unstructured data that is not only highly accessible, but also highly flexible. <br><br>

				<h3>Counterfactual estimation</h3>
				Before training the prescriptive neural networks, we must first perform a counterfactual estimation process. 
				Because the prescriptive problem's dataset only contains historical observational data, the counterfactuals are unknown, 
				e.g., the hypothetical outcomes \( y(\boldsymbol{x}_i, t) \) for \( t \neq t_{i} \). Thus, before model training, 
				we perform a counterfactual estimation step inspired by <a href="#ref-doubly-robust-policy">Dudik et al. (2014)</a>, 
				which estimates the outcomes for each observation under every treatment. <br><br>

				We aim to produce \( \Gamma \), a rewards matrix where each \( \Gamma_{i,t} \) is the estimated outcome of 
				applying treatment \( t \) to the \( i^{\text{th}} \) observation. We now review the three main methods for counterfactual estimation:
				<ol>
					<li>
						<b>Direct method.</b> T This method directly learns the outcome function \( y_{t}(\boldsymbol{x}) \)
						 by training separate models, one for each treatment \( t \). During training, each model uses 
						 only the subset of the observations that received treatment \( t \). 
						These models can be random forests or boosting methods and output an estimated outcome \( \hat{y_{t}}(\boldsymbol{x}) \) 
						for when treatment \( t \) is hypothetically applied to observation \( \boldsymbol{x} \).
					</li>
					<li>
						<b>Propensity score estimation.</b>  This method is used to calculate the probability \( p_{i,t} \) 
						that treatment \( t \) is assigned to observation \( i \). 
						The observational data \( \boldsymbol{x}_{i} \) is used to train a 
						classification model that predicts treatment assignments for each observation. 
						Then, using this model, the probability that each observation receives its prescribed treatment is calculated. 
						Here, random forests or boosting methods are also employed.
					</li>
					<li><b>Doubly robust estimation.</b> 
						Because direct estimation is often prone to treatment assignment bias, the doubly-robust 
						estimator attempts to mitigate this bias by re-weighting the estimated direct outcomes with 
						propensity score probabilities (the probability \( p_{i,t} \) that treatment \( t \) is assigned to 
						observation \( i \)). 
						This reweighting is expressed in Equation below, which calculates the 
						doubly-robust reward matrix \( \Gamma \):
						<p style="text-align: center;">
							\( \Gamma_{i,t} = \hat{y_{t}}(\boldsymbol{x}) + \frac{\mathbb{1}\{t = t_{i}\}}{p_{i,t}} \big(y_{i} - \hat{y_{t}}(\boldsymbol{x})\big). \)
						</p>
					</li>
				</ol>

				<h3>Prescription policy learning through feedforward neural networks</h3>

				<p>
					A feedforward neural network consists of layers of interconnected neurons, which are computational units that are 
					each characterized by weights 
					\( \{a_{i}\}_{i=1}^p \), a bias \( b \), and a nonlinear activation function \( h \). Given an input vector 
					\( \boldsymbol{x} \in \mathbb{R}^p \), the neuron calculates 
					\( y = \sum_{i=1}^{p}a_{i}x_{i} + b \), a weighted sum of the input's components, and passes the result \( y \) through 
					\( h \) to produce the neuron's output \( o = h(y) \) that serves as the input to the subsequent neuron. 
					As observational inputs \( \boldsymbol{x}_i \) pass through this network of interconnected neurons, this calculation is 
					performed in a nested manner, and the output of the final layer is computed, from which a prespecified loss function is 
					evaluated. Using backpropagation, the weights of the network are updated to minimize this loss function. The typical 
					structure is presented in Figure 1.
				</p>
				<figure>
					<img src="images/nn.png" alt="Feedforward Neural Network" style="width:60%;">
					<figcaption>Architecture of a feedforward neural network (adapted from [fnn-image]).</figcaption>
				</figure>
				<p>
					Without loss of generality, we assume our goal is to minimize outcomes in the prescriptive problem. The objective of our 
					prescriptive neural network is to minimize total rewards for the prescriptions \( \tau(\boldsymbol{x}_i) \) assigned to each 
					datapoint \( \boldsymbol{x}_i \) by the network:
				</p>
				<p style="text-align: center;">
					\( \min_{\tau(.)} \sum_{i=1}^{N} \sum_{t=1}^{N_t} \mathbb{1} \{\tau(\boldsymbol{x}_i) = t\} \cdot \Gamma_{i,t}. \)
				</p>
				<p>
					Because the indicator function is not differentiable, the backpropagation algorithm cannot exactly handle this objective. 
					We therefore "soften" the objective and leverage an approach analogous to that of multi-classification networks. The PNN assigns 
					treatments probabilistically, such that its output layer consists of \( N_t \) neurons, one for each distinct treatment 
					(as multi-classification networks have an output corresponding to each target class). We denote the output vector of the PNN as 
					\( \boldsymbol{z} \in \mathbb{R}^{N_t} \) and apply a softmax activation function to these output neurons to obtain a probability 
					distribution over the distinct treatments, such that \( \mathbb{P}[\tau(\boldsymbol{x}_i) = t] = \sigma_t(\boldsymbol{z}) \), 
					where \( \sigma(\cdot) \) denotes the softmax function and is defined explicitly below:
				</p>
				<p style="text-align: center;">
					\( \sigma_t(\boldsymbol{z}) = \frac{\exp{z_{t}}}{\sum_{j=1}^{T} \exp{z_{j}}} \text{ for } t=1,\ldots,T \text{ and } \boldsymbol{z} 
					\in \mathbb{R}^T. \)
				</p>
				<p>
					We obtain the final prescription of the network by finding the treatment \( t \) with the highest probability 
					\( \mathbb{P}[\tau(\boldsymbol{x}_i) = t] = \sigma_t(\boldsymbol{z}) \). This approach is analogous to a classification network, 
					where the predicted class is the one with the highest probability among the network's output nodes. The tractable objective for 
					our PNN models is therefore:
				</p>
				<p style="text-align: center;">
					\( \min_{\tau(.)} \sum_{i=1}^{N} \sum_{t=1}^{T} \mathbb{P}[\tau(\boldsymbol{x}_i) = t] \cdot \Gamma_{i,t}. \)
				</p>
			</body>
		
	  <!-- 	  
	  <center>
		
		<math xmlns="http://www.w3.org/1998/Math/MathML">
		  <mrow>
			<mrow>
			  <mo>&#x2202;</mo>
			  <mi>y</mi>
			</mrow>
			<mo>/</mo>
			<mrow>
			  <mo>&#x2202;</mo>
			  <mi>x</mi>
			</mrow>
		  </mrow>
		  <mo>=</mo>
		  <mi>x</mi>
		</math>
	  </center> -->
	  <br>
	  
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

		<!-- <div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Experiments</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div> -->

		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Experiments and Results</h1>
				Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="discussion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Discussion</h1>
				Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Conclusion</h1>
				Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1]
								Kallus, Nathan. 
								<cite>Recursive Partitioning for Personalization Using Observational Data.</cite> 
								<i>International Conference on Machine Learning</i>, PMLR, 2017.
							<br><br>
							<a id="ref_2"></a>[2]
								Bertsimas, Dimitris, Jack Dunn, and Nishanth Mundru. 
    							<cite>Optimal Prescriptive Trees.</cite> 
    							<i>INFORMS Journal on Optimization</i>, vol. 1, no. 2, 2019, pp. 164–183.
							<br><br>
							<a id="ref_3"></a>[3]
								Bertsimas, Dimitris, and Jack Dunn. 
								<cite> Optimal classification trees.</cite>
								<i>Machine Learning</i>, vol. 106, 2017, pp. 1039–1082.
							<br><br>
							<a id="ref_4"></a>[4]
								Amram, Maxime, Jack Dunn, and Ying Daisy Zhuo. 
								<cite>Optimal Policy Trees.</cite> 
								<i>Machine Learning</i>, vol. 111, no. 7, 2022, pp. 2741–2768.
							<br><br>
							<a id="ref_5"></a>[5]
								Sun, Wei, and Asterios Tsiourvas. 
								<cite>Learning Prescriptive ReLU Networks.</cite> 
								<i>International Conference on Machine Learning</i>, PMLR, 2023.
							<br><br>
							<a id="ref_6"></a>[6]
								Bergman, David, et al. 
								<cite>JANOS: An Integrated Predictive and Prescriptive Modeling Framework.</cite> 
								<i>INFORMS Journal on Computing</i>, vol. 34, no. 2, 2022, pp. 807–816.
							<br><br>
							<a id="ref_7"></a>[7]
							https://arxiv.org/pdf/2201.11838
							<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
